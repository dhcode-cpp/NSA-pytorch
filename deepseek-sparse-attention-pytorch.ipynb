{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3990ea-1fe2-4e88-a70c-fb8dea84768c",
   "metadata": {},
   "source": [
    "# DSA \n",
    "\n",
    "[DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp)\n",
    "\n",
    "[Code Source](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/raw/main/inference/model.py)\n",
    "\n",
    "**this code is DEBUGGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70aecab4-5115-4d74-9b28-360d56cf1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23df8a57-304a-46fc-8cdf-cc30ec6287dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class DSAConfig:\n",
    "    dim: int = 512\n",
    "    n_heads: int = 8\n",
    "    q_lora_rank: int = 16\n",
    "    kv_lora_rank: int = 16\n",
    "    qk_nope_head_dim: int = 16\n",
    "    qk_rope_head_dim: int = 16\n",
    "    v_head_dim: int = 16\n",
    "    index_n_heads: int = 64\n",
    "    index_head_dim: int = 128\n",
    "    index_topk: int = 2048\n",
    "    vocab_size: int = 100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34951f90-f365-4439-badd-708d43658401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim: int = config.dim\n",
    "        self.n_heads: int = config.index_n_heads\n",
    "        self.head_dim: int = config.index_head_dim\n",
    "        self.rope_head_dim: int = config.qk_rope_head_dim\n",
    "        self.index_topk: int = config.index_topk\n",
    "        self.q_lora_rank: int = config.q_lora_rank\n",
    "        \n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.head_dim) \n",
    "        self.wk = nn.Linear(self.dim, self.head_dim) \n",
    "        self.weights_proj = nn.Linear(self.dim, self.n_heads, dtype=torch.get_default_dtype()) \n",
    "\n",
    "        # self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim, dtype=torch.float8_e4m3fn), persistent=False)\n",
    "        # self.register_buffer(\"k_scale_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim // block_size, dtype=torch.float32), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, qr: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.size()\n",
    "        q = self.wq_b(qr)\n",
    "        q = q.reshape(bsz, seqlen, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        q_pe, q_nope = torch.split(q, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        # apply rope\n",
    "        q = torch.cat([q_pe, q_nope], dim=-1)\n",
    "\n",
    "        \n",
    "        k = self.wk(x)\n",
    "        k_pe, k_nope = torch.split(k, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        # apply rope\n",
    "        k = torch.cat([k_pe, k_nope], dim=-1)\n",
    "\n",
    "        weights = self.weights_proj(x) * self.n_heads ** -0.5 # bsz, head\n",
    "\n",
    "        # formula 1\n",
    "        s = q @ k.transpose(2,3) / math.sqrt(self.head_dim)\n",
    "        index_score = torch.sum(s * weights.reshpa(1,self.head_dim, 1, 1), dim = 1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            index_score += mask\n",
    "            \n",
    "        topk_indices = index_score.topk(min(self.index_topk, end_pos), dim=-1)[1]\n",
    "        topk_indices_ = topk_indices.clone()\n",
    "        return topk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1eed34-c5de-453b-b5b8-bc4d1a11c9c3",
   "metadata": {},
   "source": [
    "## MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ae25b-50b0-40b1-ac82-c6b1bbf13a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "\n",
    "        self.wq_a = nn.Linear(self.dim, self.q_lora_rank)\n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n",
    "\n",
    "        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) \n",
    "        self.wo = nn.Linear(self.n_heads * self.v_head_dim, self.dim)\n",
    "\n",
    "        self.indexer = Indexer(config)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        bsz, seq_len, _ = x.shape\n",
    "\n",
    "        q_a, kv_a = self.wq_a(x), self.wkv_a(x)\n",
    "        c_a, k_pe = torch.split(kv_a, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        \n",
    "        q, kv = self.wq_b(q_a), self.wkv_b(c_a)\n",
    "        q = q.view(bsz, seqlen, self.n_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "\n",
    "        kv = kv.view(bsz, seqlen, self.n_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "        k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "\n",
    "        q = torch.cat([q_nope, q_pe], dim=-1)\n",
    "        k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_heads, -1)], dim=-1)\n",
    "\n",
    "        scores = q @ k.reshape(2,3)\n",
    "\n",
    "        indexer_mask = self.indexer(x, qr)\n",
    "        scores = scores + (mask + indexer_mask).unsqueeze(dim = 1)\n",
    "\n",
    "        p = F.softmax(scores, dim = -1) \n",
    "        z = p @ v\n",
    "        z = z.reshape(bsz, seq_len, n_heads * self.v_head_dim)\n",
    "        z = self.wo(z)\n",
    "\n",
    "        return z"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
