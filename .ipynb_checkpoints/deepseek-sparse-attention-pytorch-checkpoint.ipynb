{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3990ea-1fe2-4e88-a70c-fb8dea84768c",
   "metadata": {},
   "source": [
    "# DSA \n",
    "\n",
    "[DeepSeek-V3.2-Exp](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp)\n",
    "\n",
    "[Code Source](https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/raw/main/inference/model.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70aecab4-5115-4d74-9b28-360d56cf1622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23df8a57-304a-46fc-8cdf-cc30ec6287dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass \n",
    "class DSAConfig:\n",
    "    dim: int = 512\n",
    "    n_heads: int = 8\n",
    "    q_lora_rank: int = 16\n",
    "    kv_lora_rank: int = 16\n",
    "    qk_nope_head_dim: int = 16\n",
    "    qk_rope_head_dim: int = 16\n",
    "    v_head_dim: int = 16\n",
    "    index_n_heads: int = 64\n",
    "    index_head_dim: int = 128\n",
    "    index_topk: int = 2048\n",
    "    vocab_size: int = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34951f90-f365-4439-badd-708d43658401",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim: int = config.dim\n",
    "        self.n_heads: int = config.index_n_heads\n",
    "        self.head_dim: int = config.index_head_dim\n",
    "        self.rope_head_dim: int = config.qk_rope_head_dim\n",
    "        self.index_topk: int = config.index_topk\n",
    "        self.q_lora_rank: int = config.q_lora_rank\n",
    "        \n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.head_dim) \n",
    "        self.wk = nn.Linear(self.dim, self.head_dim) \n",
    "        self.weights_proj = nn.Linear(self.dim, self.n_heads, dtype=torch.get_default_dtype()) \n",
    "\n",
    "        # self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim, dtype=torch.float8_e4m3fn), persistent=False)\n",
    "        # self.register_buffer(\"k_scale_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim // block_size, dtype=torch.float32), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, qr: torch.Tensor):\n",
    "        bsz, seqlen, _ = x.size()\n",
    "        q = self.wq_b(qr)\n",
    "        q = q.reshape(bsz, seqlen, self.n_heads, self.head_dim).transpose(1,2)\n",
    "        \n",
    "        q_pe, q_nope = torch.split(q, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        # apply rope\n",
    "        q = torch.cat([q_pe, q_nope], dim=-1)\n",
    "\n",
    "        \n",
    "        k = self.wk(x)\n",
    "        k_pe, k_nope = torch.split(k, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        # apply rope\n",
    "        k = torch.cat([k_pe, k_nope], dim=-1)\n",
    "\n",
    "        weights = self.weights_proj(x) * self.n_heads ** -0.5 # bsz, head\n",
    "\n",
    "        # formula 1\n",
    "        s = q @ k.transpose(2,3) / math.sqrt(self.head_dim)\n",
    "        index_score = torch.sum(s * weights.reshpa(1,self.head_dim, 1, 1), dim = 1)\n",
    "        \n",
    "        if mask is not None:\n",
    "            index_score += mask\n",
    "            \n",
    "        topk_indices = index_score.topk(min(self.index_topk, end_pos), dim=-1)[1]\n",
    "        topk_indices_ = topk_indices.clone()\n",
    "        return topk_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1eed34-c5de-453b-b5b8-bc4d1a11c9c3",
   "metadata": {},
   "source": [
    "## MLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2ae25b-50b0-40b1-ac82-c6b1bbf13a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dim = config.dim\n",
    "        self.n_heads = config.n_heads\n",
    "        self.q_lora_rank = config.q_lora_rank\n",
    "        self.kv_lora_rank = config.kv_lora_rank\n",
    "        self.qk_nope_head_dim = config.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = config.qk_rope_head_dim\n",
    "        self.qk_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "        self.v_head_dim = config.v_head_dim\n",
    "\n",
    "        self.wq_a = nn.Linear(self.dim, self.q_lora_rank)\n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n",
    "\n",
    "        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) \n",
    "        self.wo = nn.Linear(self.n_heads * self.v_head_dim, self.dim)\n",
    "\n",
    "        self.indexer = Indexer(config)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        bsz, seq_len, _ = x.shape\n",
    "\n",
    "        q_a, kv_a = self.wq_a(x), self.wkv_a(x)\n",
    "        c_a, k_pe = torch.split(kv_a, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        \n",
    "        q, kv = self.wq_b(q_a), self.wkv_b(c_a)\n",
    "        q = q.view(bsz, seqlen, self.n_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "\n",
    "        kv = kv.view(bsz, seqlen, self.n_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "        k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "\n",
    "        q = torch.cat([q_nope, q_pe], dim=-1)\n",
    "        k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_heads, -1)], dim=-1)\n",
    "\n",
    "        scores = q @ k.reshape(2,3)\n",
    "\n",
    "        p = F.softmax(dim = -1) \n",
    "        z = p @ v\n",
    "        z = z.reshape(bsz, seq_len, n_heads * self.v_head_dim)\n",
    "        z = self.wo(z)\n",
    "\n",
    "        return z\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44763e5d-e38e-43a6-a8d0-3d1122acd3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8aba1e-f5db-4265-b35e-fbd90762a4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.q_lora_rank = args.q_lora_rank\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        self.qk_nope_head_dim = args.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = args.qk_rope_head_dim\n",
    "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n",
    "        self.v_head_dim = args.v_head_dim\n",
    "\n",
    "        # MLA Q\n",
    "        self.wq_a = nn.Linear(self.dim, self.q_lora_rank)\n",
    "        self.q_norm = RMSNorm(self.q_lora_rank) # QK Norm 作用于 latent\n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n",
    "\n",
    "        # MLA KV\n",
    "        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank) # QK Norm 作用于 latent\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank,  \n",
    "                               self.n_heads * (self.qk_nope_head_dim + self.v_head_dim)) # 多头 KV\n",
    "        self.wo = nn.Linear(self.n_heads * self.v_head_dim, self.dim)\n",
    "\n",
    "        # self.indexer = Indexer(args)\n",
    "        \n",
    "        self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n",
    "        self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)\n",
    "\n",
    "    def forward(x):\n",
    "        \n",
    "        bsz, seqlen, _ = x.size()\n",
    "        end_pos = start_pos + seqlen\n",
    "\n",
    "        # Q\n",
    "        qr = self.q_norm(self.wq_a(x))\n",
    "        q = self.wq_b(qr)\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        # q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "\n",
    "        # KV and Cache\n",
    "        kv = self.wkv_a(x)\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        kv = self.kv_norm(kv)\n",
    "        # k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n",
    "        self.kv_cache[:bsz, start_pos:end_pos] = kv\n",
    "        self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "\n",
    "        \n",
    "        if mask is not None:    # MHA prefill\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1) # MLA 使用完整的 q\n",
    "            kv = self.wkv_b(kv)\n",
    "            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q.float(), k.float()) # * self.softmax_scale\n",
    "\n",
    "            # indexer\n",
    "            # # Prefill 时基于 top-k index mask 来处理 score\n",
    "            # topk_indices = self.indexer(x, qr, start_pos, freqs_cis, mask) # 传入 x 和 x q 的低秩版本\n",
    "            # index_mask = torch.full((bsz, seqlen, seqlen), float(\"-inf\"), device=x.device).scatter_(-1, topk_indices, 0)\n",
    "            # index_mask += mask\n",
    "            # scores += index_mask.unsqueeze(2)\n",
    "\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores.type_as(x), v) # p @ v\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdbf35a-f890-4db8-b3ed-52b2e2f07738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8551da15-dbb6-477f-8c78-d83df53bf519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0837d3-31c2-49fe-9efc-642276fb8e5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a45241b6-27cf-4df3-9931-cbf5c58e679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, Literal\n",
    "\n",
    "from einops import rearrange\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from kernel import act_quant, fp8_gemm, fp8_index # kernel 是什么\n",
    "\n",
    "world_size = 1\n",
    "rank = 0\n",
    "block_size = 128\n",
    "\n",
    "@dataclass\n",
    "class ModelArgs:\n",
    "    \"\"\"\n",
    "    Data class for defining model arguments and hyperparameters.\n",
    "\n",
    "    Attributes:\n",
    "        max_batch_size (int): Maximum batch size.\n",
    "        max_seq_len (int): Maximum sequence length.\n",
    "        dtype (Literal[\"bf16\", \"fp8\"]): Data type for computations.\n",
    "        scale_fmt (Optional[str]): Format for quantization scale.\n",
    "        vocab_size (int): Vocabulary size.\n",
    "        dim (int): Model dimension.\n",
    "        inter_dim (int): Intermediate dimension for MLP layers.\n",
    "        moe_inter_dim (int): Intermediate dimension for MoE layers.\n",
    "        n_layers (int): Number of transformer layers.\n",
    "        n_dense_layers (int): Number of dense layers in the model.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_routed_experts (int): Number of routed experts for MoE layers.\n",
    "        n_shared_experts (int): Number of shared experts for MoE layers.\n",
    "        n_activated_experts (int): Number of activated experts in MoE layers.\n",
    "        n_expert_groups (int): Number of expert groups.\n",
    "        n_limited_groups (int): Number of limited groups for MoE routing.\n",
    "        score_func (Literal[\"softmax\", \"sigmoid\"]): Scoring function for MoE routing.\n",
    "        route_scale (float): Scaling factor for routing scores.\n",
    "        q_lora_rank (int): LoRA rank for query projections.\n",
    "        kv_lora_rank (int): LoRA rank for key-value projections.\n",
    "        qk_nope_head_dim (int): Dimension for query-key projections without positional embeddings.\n",
    "        qk_rope_head_dim (int): Dimension for query-key projections with rotary embeddings.\n",
    "        v_head_dim (int): Dimension for value projections.\n",
    "        original_seq_len (int): Original sequence length.\n",
    "        rope_theta (float): Base for rotary positional encoding.\n",
    "        rope_factor (float): Scaling factor for extended sequence lengths.\n",
    "        beta_fast (int): Fast beta correction factor.\n",
    "        beta_slow (int): Slow beta correction factor.\n",
    "        mscale (float): Scaling factor for extended attention.\n",
    "        index_head_dim (int): Dimension for index head.\n",
    "        index_topk (int): Top-k for index head.\n",
    "    \"\"\"\n",
    "    max_batch_size: int = 8\n",
    "    max_seq_len: int = 4096 * 4\n",
    "    dtype: Literal[\"bf16\", \"fp8\"] = \"bf16\"\n",
    "    scale_fmt: Optional[str] = None\n",
    "    vocab_size: int = 102400\n",
    "    dim: int = 2048\n",
    "    inter_dim: int = 10944\n",
    "    moe_inter_dim: int = 1408\n",
    "    n_layers: int = 27\n",
    "    n_dense_layers: int = 1\n",
    "    n_heads: int = 16\n",
    "    # moe\n",
    "    n_routed_experts: int = 64\n",
    "    n_shared_experts: int = 2\n",
    "    n_activated_experts: int = 6\n",
    "    n_expert_groups: int = 1\n",
    "    n_limited_groups: int = 1\n",
    "    score_func: Literal[\"softmax\", \"sigmoid\"] = \"softmax\"\n",
    "    route_scale: float = 1.\n",
    "    # mla\n",
    "    q_lora_rank: int = 512\n",
    "    kv_lora_rank: int = 512\n",
    "    qk_nope_head_dim: int = 128\n",
    "    qk_rope_head_dim: int = 64\n",
    "    v_head_dim: int = 128\n",
    "    # yarn\n",
    "    original_seq_len: int = 4096\n",
    "    rope_theta: float = 10000.0\n",
    "    rope_factor: float = 40\n",
    "    beta_fast: int = 32\n",
    "    beta_slow: int = 1\n",
    "    mscale: float = 1.\n",
    "    # index\n",
    "    index_n_heads: int = 64\n",
    "    index_head_dim: int = 128\n",
    "    index_topk: int = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "72a71728-f2d6-4dcf-988f-1782344d0a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def apply_rotary_emb(x: torch.Tensor, freqs_cis: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies rotary positional embeddings to the input tensor.\n",
    "\n",
    "    Args:\n",
    "        x (torch.Tensor): Input tensor with positional embeddings to be applied.\n",
    "        freqs_cis (torch.Tensor): Precomputed complex exponential values for positional embeddings.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with rotary embeddings applied.\n",
    "    \"\"\"\n",
    "    dtype = x.dtype\n",
    "    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))\n",
    "    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))\n",
    "    y = torch.view_as_real(x * freqs_cis).flatten(3)\n",
    "    return y.to(dtype)\n",
    "\n",
    "\n",
    "# 这是什么？\n",
    "def rotate_activation(x: torch.Tensor) -> torch.Tensor:\n",
    "    assert x.dtype == torch.bfloat16\n",
    "    from fast_hadamard_transform import hadamard_transform\n",
    "    hidden_size = x.size(-1)\n",
    "    return hadamard_transform(x, scale=hidden_size ** -0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4faa4ef4-4e84-4dd4-9045-c21fd4ce8db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer Normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim, dtype=torch.float32))\n",
    "        self.bias = nn.Parameter(torch.zeros(dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return F.layer_norm(x.float(), (self.dim,), self.weight, self.bias, self.eps).type_as(x)\n",
    "\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Root Mean Square Layer Normalization (RMSNorm).\n",
    "\n",
    "    Args:\n",
    "        dim (int): Dimension of the input tensor.\n",
    "        eps (float): Epsilon value for numerical stability. Defaults to 1e-6.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, eps: float = 1e-6):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x: torch.Tensor, residual: Optional[torch.Tensor] = None):\n",
    "        \"\"\"\n",
    "        Forward pass for RMSNorm.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Normalized tensor with the same shape as input.\n",
    "        \"\"\"\n",
    "        dtype = x.dtype\n",
    "        if residual is None:\n",
    "            x = x.float()\n",
    "            var = x.pow(2).mean(-1, keepdim=True)\n",
    "            x = x * torch.rsqrt(var + self.eps)\n",
    "            return (self.weight * x).to(dtype)\n",
    "        else:\n",
    "            x = residual = x.float() + residual.float()\n",
    "            var = x.pow(2).mean(-1, keepdim=True)\n",
    "            x = x * torch.rsqrt(var + self.eps)\n",
    "            return (self.weight * x).to(dtype), residual.to(dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0ed0cee-4b22-4ef5-9acb-bfd9ccf6e1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Indexer(torch.nn.Module):\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim: int = args.dim\n",
    "        self.n_heads: int = args.index_n_heads\n",
    "        self.n_local_heads = args.index_n_heads // world_size\n",
    "        self.head_dim: int = args.index_head_dim\n",
    "        self.rope_head_dim: int = args.qk_rope_head_dim\n",
    "        self.index_topk: int = args.index_topk\n",
    "        self.q_lora_rank: int = args.q_lora_rank\n",
    "        \n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.head_dim) # 独立的参数处理 index 分支, 多头\n",
    "        self.wk = nn.Linear(self.dim, self.head_dim) # 单头\n",
    "        self.k_norm = LayerNorm(self.head_dim)\n",
    "        \n",
    "        self.weights_proj = nn.Linear(self.dim, self.n_heads, dtype=torch.get_default_dtype()) # 哪个头更加重要？\n",
    "\n",
    "        \n",
    "        self.softmax_scale = self.head_dim ** -0.5\n",
    "        self.scale_fmt = args.scale_fmt\n",
    "\n",
    "        self.register_buffer(\"k_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim, dtype=torch.float8_e4m3fn), persistent=False)\n",
    "        self.register_buffer(\"k_scale_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.head_dim // block_size, dtype=torch.float32), persistent=False)\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor, qr: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        bsz, seqlen, _ = x.size()\n",
    "        end_pos = start_pos + seqlen\n",
    "        q = self.wq_b(qr)\n",
    "        q = rearrange(q, 'b s (h d) -> b s h d', d=self.head_dim)\n",
    "        q_pe, q_nope = torch.split(q, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "\n",
    "        \n",
    "        q = torch.cat([q_pe, q_nope], dim=-1)\n",
    "        k = self.wk(x)\n",
    "        k = self.k_norm(k)\n",
    "        k_pe, k_nope = torch.split(k, [self.rope_head_dim, self.head_dim - self.rope_head_dim], dim=-1)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis).squeeze(2)\n",
    "        k = torch.cat([k_pe, k_nope], dim=-1)\n",
    "\n",
    "        # 作用? apply rope?\n",
    "        q = rotate_activation(q)\n",
    "        k = rotate_activation(k)\n",
    "\n",
    "        \n",
    "        q_fp8, q_scale = act_quant(q, block_size, self.scale_fmt) # 激活什么?\n",
    "        k_fp8, k_scale = act_quant(k, block_size, self.scale_fmt)\n",
    "\n",
    "        \n",
    "        self.k_cache[:bsz, start_pos:end_pos] = k_fp8\n",
    "        self.k_scale_cache[:bsz, start_pos:end_pos] = k_scale # 为什么要有 scale\n",
    "\n",
    "        \n",
    "        weights = self.weights_proj(x) * self.n_heads ** -0.5 # bsz, head\n",
    "        weights = weights.unsqueeze(-1) * q_scale * self.softmax_scale # bsz, head, 1\n",
    "        \n",
    "        index_score = fp8_index(q_fp8.contiguous(), weights, self.k_cache[:bsz, :end_pos].contiguous(), self.k_scale_cache[:bsz, :end_pos].contiguous())\n",
    "\n",
    "        \n",
    "        if mask is not None:\n",
    "            index_score += mask\n",
    "            \n",
    "        topk_indices = index_score.topk(min(self.index_topk, end_pos), dim=-1)[1]\n",
    "        topk_indices_ = topk_indices.clone()\n",
    "        dist.broadcast(topk_indices_, src=0)\n",
    "        assert torch.all(topk_indices == topk_indices_), f\"{topk_indices=} {topk_indices_=}\"\n",
    "        return topk_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bfe0efc-4a29-4c82-8a63-cef967a32e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MLA(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Latent Attention (MLA) Layer.\n",
    "\n",
    "    Attributes:\n",
    "        dim (int): Dimensionality of the input features.\n",
    "        n_heads (int): Number of attention heads.\n",
    "        n_local_heads (int): Number of local attention heads for distributed systems.\n",
    "        q_lora_rank (int): Rank for low-rank query projection.\n",
    "        kv_lora_rank (int): Rank for low-rank key/value projection.\n",
    "        qk_nope_head_dim (int): Dimensionality of non-positional query/key projections.\n",
    "        qk_rope_head_dim (int): Dimensionality of rotary-positional query/key projections.\n",
    "        qk_head_dim (int): Total dimensionality of query/key projections.\n",
    "        v_head_dim (int): Dimensionality of value projections.\n",
    "        softmax_scale (float): Scaling factor for softmax in attention computation.\n",
    "    \"\"\"\n",
    "    def __init__(self, args: ModelArgs):\n",
    "        super().__init__()\n",
    "        self.dim = args.dim\n",
    "        self.n_heads = args.n_heads\n",
    "        self.n_local_heads = args.n_heads // world_size\n",
    "        self.q_lora_rank = args.q_lora_rank\n",
    "        self.kv_lora_rank = args.kv_lora_rank\n",
    "        self.qk_nope_head_dim = args.qk_nope_head_dim\n",
    "        self.qk_rope_head_dim = args.qk_rope_head_dim\n",
    "        self.qk_head_dim = args.qk_nope_head_dim + args.qk_rope_head_dim\n",
    "        self.v_head_dim = args.v_head_dim\n",
    "\n",
    "        self.wq_a = nn.Linear(self.dim, self.q_lora_rank)\n",
    "        self.q_norm = RMSNorm(self.q_lora_rank)\n",
    "        self.wq_b = nn.Linear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n",
    "        \n",
    "        self.wkv_a = nn.Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
    "        self.wkv_b = nn.Linear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n",
    "        self.wo = nn.Linear(self.n_heads * self.v_head_dim, self.dim)\n",
    "\n",
    "        # Original Distributed Version\n",
    "        # self.wq_a = Linear(self.dim, self.q_lora_rank)\n",
    "        # self.q_norm = RMSNorm(self.q_lora_rank)\n",
    "        # self.wq_b = ColumnParallelLinear(self.q_lora_rank, self.n_heads * self.qk_head_dim)\n",
    "        # self.wkv_a = Linear(self.dim, self.kv_lora_rank + self.qk_rope_head_dim)\n",
    "        # self.kv_norm = RMSNorm(self.kv_lora_rank)\n",
    "        # self.wkv_b = ColumnParallelLinear(self.kv_lora_rank, self.n_heads * (self.qk_nope_head_dim + self.v_head_dim))\n",
    "        # self.wo = RowParallelLinear(self.n_heads * self.v_head_dim, self.dim)\n",
    "\n",
    "        \n",
    "        self.softmax_scale = self.qk_head_dim ** -0.5\n",
    "        if args.max_seq_len > args.original_seq_len:\n",
    "            mscale = 0.1 * args.mscale * math.log(args.rope_factor) + 1.0\n",
    "            self.softmax_scale = self.softmax_scale * mscale * mscale\n",
    "\n",
    "        self.indexer = Indexer(args)\n",
    "\n",
    "        self.register_buffer(\"kv_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.kv_lora_rank), persistent=False)\n",
    "        self.register_buffer(\"pe_cache\", torch.zeros(args.max_batch_size, args.max_seq_len, self.qk_rope_head_dim), persistent=False)\n",
    "        self.dequant_wkv_b = None\n",
    "\n",
    "    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):\n",
    "        \"\"\"\n",
    "        Forward pass for the Multi-Head Latent Attention (MLA) Layer.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (batch_size, seq_len, dim).\n",
    "            start_pos (int): Starting position in the sequence for caching.\n",
    "            freqs_cis (torch.Tensor): Precomputed complex exponential values for rotary embeddings.\n",
    "            mask (Optional[torch.Tensor]): Mask tensor to exclude certain positions from attention.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor with the same shape as the input.\n",
    "        \"\"\"\n",
    "        bsz, seqlen, _ = x.size()\n",
    "        end_pos = start_pos + seqlen\n",
    "\n",
    "        # Q\n",
    "        qr = self.q_norm(self.wq_a(x))\n",
    "        q = self.wq_b(qr)\n",
    "        q = q.view(bsz, seqlen, self.n_local_heads, self.qk_head_dim)\n",
    "        q_nope, q_pe = torch.split(q, [self.qk_nope_head_dim, self.qk_rope_head_dim], dim=-1)\n",
    "        q_pe = apply_rotary_emb(q_pe, freqs_cis)\n",
    "\n",
    "        # KV and Cache\n",
    "        kv = self.wkv_a(x)\n",
    "        kv, k_pe = torch.split(kv, [self.kv_lora_rank, self.qk_rope_head_dim], dim=-1)\n",
    "        kv = self.kv_norm(kv)\n",
    "        k_pe = apply_rotary_emb(k_pe.unsqueeze(2), freqs_cis)\n",
    "        self.kv_cache[:bsz, start_pos:end_pos] = kv\n",
    "        self.pe_cache[:bsz, start_pos:end_pos] = k_pe.squeeze(2)\n",
    "\n",
    "        \n",
    "        if mask is not None:    # MHA prefill\n",
    "            q = torch.cat([q_nope, q_pe], dim=-1) # MLA 使用完整的 q\n",
    "            kv = self.wkv_b(kv)\n",
    "            kv = kv.view(bsz, seqlen, self.n_local_heads, self.qk_nope_head_dim + self.v_head_dim)\n",
    "            k_nope, v = torch.split(kv, [self.qk_nope_head_dim, self.v_head_dim], dim=-1)\n",
    "            k = torch.cat([k_nope, k_pe.expand(-1, -1, self.n_local_heads, -1)], dim=-1)\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q.float(), k.float()) * self.softmax_scale\n",
    "\n",
    "            # indexer\n",
    "            # Prefill 时基于 top-k index mask 来处理 score\n",
    "            \n",
    "            topk_indices = self.indexer(x, qr, start_pos, freqs_cis, mask) # 传入 x 和 x q 的低秩版本\n",
    "            index_mask = torch.full((bsz, seqlen, seqlen), float(\"-inf\"), device=x.device).scatter_(-1, topk_indices, 0)\n",
    "            index_mask += mask\n",
    "            scores += index_mask.unsqueeze(2)\n",
    "\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores.type_as(x), v) # p @ v\n",
    "        else:                   # MHA decode, 非 MQA 版本\n",
    "            wkv_b = self.wkv_b.weight if self.dequant_wkv_b is None else self.dequant_wkv_b\n",
    "            wkv_b = wkv_b.view(self.n_local_heads, -1, self.kv_lora_rank)\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim]) # 使用 nope q\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope.float(), self.kv_cache[:bsz, :end_pos].float()) +\n",
    "                      torch.einsum(\"bshr,btr->bsht\", q_pe.float(), self.pe_cache[:bsz, :end_pos].float())) * self.softmax_scale\n",
    "\n",
    "            # indexer\n",
    "            topk_indices = self.indexer(x, qr, start_pos, freqs_cis, mask)\n",
    "\n",
    "            # torch.full \n",
    "            index_mask = torch.full((bsz, 1, end_pos), float(\"-inf\"), device=x.device).scatter_(-1, topk_indices, 0)\n",
    "            scores += index_mask.unsqueeze(2) # bsz, 1, 1, seq_len\n",
    "\n",
    "            scores = scores.softmax(dim=-1, dtype=torch.float32)\n",
    "            x = torch.einsum(\"bsht,btc->bshc\", scores.type_as(x), self.kv_cache[:bsz, :end_pos])\n",
    "            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:]) # 增加投影出去\n",
    "        x = self.wo(x.flatten(2))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "75081c30-35b9-4f38-ad98-26e6ab2cf053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 10])\n",
      "torch.Size([2, 1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((2, 1, 10), float(\"-inf\"))\n",
    "print(a.shape)\n",
    "print(a.unsqueeze(2).shape) # bs, head, 1, seq_len,    中间两个维度: 单头 score, 单个 q, 非 batch decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2cf44e7-d2ce-47a1-87f9-0b15377488c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLA(\n",
      "  (wq_a): Linear(in_features=2048, out_features=0, bias=True)\n",
      "  (q_norm): RMSNorm()\n",
      "  (wq_b): Linear(in_features=0, out_features=3072, bias=True)\n",
      "  (wkv_a): Linear(in_features=2048, out_features=576, bias=True)\n",
      "  (kv_norm): RMSNorm()\n",
      "  (wkv_b): Linear(in_features=512, out_features=4096, bias=True)\n",
      "  (wo): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (indexer): Indexer(\n",
      "    (wq_b): Linear(in_features=0, out_features=8192, bias=True)\n",
      "    (wk): Linear(in_features=2048, out_features=128, bias=True)\n",
      "    (k_norm): LayerNorm()\n",
      "    (weights_proj): Linear(in_features=2048, out_features=64, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args = ModelArgs()\n",
    "model = MLA(args)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e4e290c7-53d7-48af-bb6f-6875d98c2d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 4\n",
    "seq_len = 8\n",
    "dim = 2048\n",
    "x = torch.randn( bs, seq_len, dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a1ca325-db33-4b28-9bdd-ba72cbba4901",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 8, 1, 32]' is invalid for input of size 524288",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq_cis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/xr1/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/xr1/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[20], line 77\u001b[0m, in \u001b[0;36mMLA.forward\u001b[0;34m(self, x, start_pos, freqs_cis, mask)\u001b[0m\n\u001b[1;32m     75\u001b[0m q \u001b[38;5;241m=\u001b[39m q\u001b[38;5;241m.\u001b[39mview(bsz, seqlen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_local_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_head_dim)\n\u001b[1;32m     76\u001b[0m q_nope, q_pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(q, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_nope_head_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_rope_head_dim], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m q_pe \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_pe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreqs_cis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m kv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwkv_a(x)\n\u001b[1;32m     79\u001b[0m kv, k_pe \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msplit(kv, [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkv_lora_rank, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqk_rope_head_dim], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[29], line 14\u001b[0m, in \u001b[0;36mapply_rotary_emb\u001b[0;34m(x, freqs_cis)\u001b[0m\n\u001b[1;32m     12\u001b[0m dtype \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m     13\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_complex(x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m---> 14\u001b[0m freqs_cis \u001b[38;5;241m=\u001b[39m \u001b[43mfreqs_cis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(x \u001b[38;5;241m*\u001b[39m freqs_cis)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\u001b[38;5;241m.\u001b[39mto(dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 8, 1, 32]' is invalid for input of size 524288"
     ]
    }
   ],
   "source": [
    "model(x, 0, freq_cis, mask = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4523f70c-33a1-4128-8d29-134eb7cc4183",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def precompute_freqs_cis(args: ModelArgs) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Precomputes frequency-based complex exponential values for rotary positional embeddings.\n",
    "\n",
    "    Args:\n",
    "        args (ModelArgs): Model arguments containing positional embedding parameters.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Precomputed complex exponential values for positional embeddings.\n",
    "    \"\"\"\n",
    "    dim = args.qk_rope_head_dim\n",
    "    seqlen = args.max_seq_len\n",
    "    beta_fast = args.beta_fast\n",
    "    beta_slow = args.beta_slow\n",
    "    base = args.rope_theta\n",
    "    factor = args.rope_factor\n",
    "\n",
    "    def find_correction_dim(num_rotations, dim, base, max_seq_len):\n",
    "        \"\"\"\n",
    "        Computes the correction dimension for a given number of rotations in the rotary positional embedding.\n",
    "\n",
    "        Args:\n",
    "            num_rotations (float): Number of rotations to compute the correction for.\n",
    "            dim (int): Dimensionality of the embedding space.\n",
    "            base (float): Base value for the exponential computation.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "            float: The correction dimension based on the input parameters.\n",
    "        \"\"\"\n",
    "        return dim * math.log(max_seq_len / (num_rotations * 2 * math.pi)) / (2 * math.log(base))\n",
    "\n",
    "    def find_correction_range(low_rot, high_rot, dim, base, max_seq_len):\n",
    "        \"\"\"\n",
    "        Computes the range of correction dimensions for rotary positional embeddings.\n",
    "\n",
    "        Args:\n",
    "            low_rot (float): Lower bound for the number of rotations.\n",
    "            high_rot (float): Upper bound for the number of rotations.\n",
    "            dim (int): Dimensionality of the embedding space.\n",
    "            base (float): Base value for the exponential computation.\n",
    "            max_seq_len (int): Maximum sequence length.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[int, int]: The range of correction dimensions (low, high), clamped to valid indices.\n",
    "        \"\"\"\n",
    "        low = math.floor(find_correction_dim(low_rot, dim, base, max_seq_len))\n",
    "        high = math.ceil(find_correction_dim(high_rot, dim, base, max_seq_len))\n",
    "        return max(low, 0), min(high, dim-1)\n",
    "\n",
    "    def linear_ramp_factor(min, max, dim):\n",
    "        \"\"\"\n",
    "        Computes a linear ramp function used to smooth values between a minimum and maximum range.\n",
    "\n",
    "        Args:\n",
    "            min (float): Minimum value for the ramp function.\n",
    "            max (float): Maximum value for the ramp function.\n",
    "            dim (int): Dimensionality of the ramp tensor.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of shape (dim,) with values linearly interpolated between 0 and 1,\n",
    "                clamped to the range [0, 1].\n",
    "        \"\"\"\n",
    "        if min == max:\n",
    "            max += 0.001\n",
    "        linear_func = (torch.arange(dim, dtype=torch.float32) - min) / (max - min)\n",
    "        ramp_func = torch.clamp(linear_func, 0, 1)\n",
    "        return ramp_func\n",
    "\n",
    "    freqs = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.float32) / dim))\n",
    "    if seqlen > args.original_seq_len:\n",
    "        low, high = find_correction_range(beta_fast, beta_slow, dim, base, args.original_seq_len)\n",
    "        smooth = 1 - linear_ramp_factor(low, high, dim // 2)\n",
    "        freqs = freqs / factor * (1 - smooth) + freqs * smooth\n",
    "\n",
    "    t = torch.arange(seqlen)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return freqs_cis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc8b27e0-e36a-4db1-bfe6-1ac5c444088f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16384, 32])\n"
     ]
    }
   ],
   "source": [
    "freq_cis = precompute_freqs_cis(args)\n",
    "print(freq_cis.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98782be9-acdb-41d8-8c4e-40637abdbdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "        if mask is not None:    # MHA prefill\n",
    "            # ...\n",
    "            scores = torch.einsum(\"bshd,bthd->bsht\", q.float(), k.float()) * self.softmax_scale\n",
    "            # ...\n",
    "            x = torch.einsum(\"bsht,bthd->bshd\", scores.type_as(x), v) # p @ v\n",
    "        else:                   # MHA decode\n",
    "            # ...\n",
    "\n",
    "            # W^{UK}_i\n",
    "            q_nope = torch.einsum(\"bshd,hdc->bshc\", q_nope, wkv_b[:, :self.qk_nope_head_dim]) # 使用 nope q\n",
    "            scores = (torch.einsum(\"bshc,btc->bsht\", q_nope.float(), self.kv_cache[:bsz, :end_pos].float()) +\n",
    "                      torch.einsum(\"bshr,btr->bsht\", q_pe.float(), self.pe_cache[:bsz, :end_pos].float())) * self.softmax_scale\n",
    "            # ...\n",
    "            x = torch.einsum(\"bsht,btc->bshc\", scores.type_as(x), self.kv_cache[:bsz, :end_pos])\n",
    "            # W^{UV}_i\n",
    "            x = torch.einsum(\"bshc,hdc->bshd\", x, wkv_b[:, -self.v_head_dim:]) # 增加投影出去\n",
    "        x = self.wo(x.flatten(2))\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
